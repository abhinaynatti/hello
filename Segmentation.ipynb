{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Segmentation",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhinaynatti/hello/blob/master/Segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fLkA0UaQfvhq",
        "colab": {}
      },
      "source": [
        "# !pip install --upgrade tensorflow-gpu\n",
        "# from google.colab import drive; drive.mount('/content/drive');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ejbrTknQfsSW",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, DepthwiseConv2D, BatchNormalization, Dropout, ReLU, UpSampling2D\n",
        "import tensorflow as tf;   from tensorflow import keras;   from tensorflow.keras.models import Model\n",
        "import numpy as np; import pandas as pd;  import os, time; from zipfile import ZipFile; import glob;\n",
        "import matplotlib as mpl; import matplotlib.pyplot as plt; from pathlib import Path;\n",
        "from os import listdir; import scipy.misc; from IPython.display import clear_output;\n",
        "\n",
        "mpl.rcParams['axes.grid'] = False; mpl.rcParams['figure.figsize'] = (12, 12);\n",
        "get_ipython().run_line_magic('matplotlib', 'inline')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zm89Qz5RfsSe",
        "outputId": "aad55477-92eb-438f-b4e2-ad42b88f86d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "is_gpu_available = tf.test.is_gpu_available(cuda_only = False, min_cuda_compute_capability = None)\n",
        "print(tf.__version__); print(f\"GPU is available: {is_gpu_available}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n",
            "GPU is available: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XlrlmkB5fsSm"
      },
      "source": [
        "### Inverted Residual Block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Lu3Y1vYkr0TN",
        "colab": {}
      },
      "source": [
        "class Conv2D_BN(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, nb_out_filters, kernel_sz, stride_sz, padding = \"same\", dilation_rate = 1, momentum = 0.99, bool_apply_act = True, \n",
        "                 bool_apply_norm = True):\n",
        "\n",
        "        super(Conv2D_BN, self).__init__(); self.use_bias = not bool_apply_norm;\n",
        "        self.bool_apply_act = bool_apply_act; self.bool_apply_norm = bool_apply_norm; \n",
        "        \n",
        "        self.conv_layer = Conv2D(filters = nb_out_filters, kernel_size = kernel_sz, strides = stride_sz, padding = padding, \n",
        "                                 dilation_rate = dilation_rate, use_bias = self.use_bias);\n",
        "        self.batch_norm_layer = BatchNormalization(axis = -1, momentum = momentum)\n",
        "        self.relu6_layer = ReLU(max_value = 6.0, threshold = 0.0)\n",
        "\n",
        "\n",
        "    def call(self, input_tensor, training = True):\n",
        "\n",
        "        x = self.conv_layer(input_tensor);\n",
        "        if self.bool_apply_norm: x = self.batch_norm_layer(x, training = training);\n",
        "        if self.bool_apply_act:  x = self.relu6_layer(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PMncMZ2QfsSn",
        "colab": {}
      },
      "source": [
        "class InvertedResidual(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, nb_out_filters, nb_inp_filters, kernel_sz = 3, stride_sz = 1, dilation_rate = 1, exp_ratio = 6, width_multiplier = 1):\n",
        "        \n",
        "        super(InvertedResidual, self).__init__();\n",
        "        self.exp_ratio = exp_ratio; self.width_multiplier = width_multiplier;\n",
        "        \n",
        "        self.res_connect = True if (stride_sz == 1 and nb_out_filters == nb_inp_filters) else False;\n",
        "        nb_out_filters = nb_out_filters * self.width_multiplier; nb_bottleneck_filters = self.exp_ratio * nb_inp_filters;\n",
        "        \n",
        "        self.conv_pxp_1 = Conv2D_BN(nb_bottleneck_filters, 1, 1);\n",
        "        self.conv_pxp_2 = Conv2D_BN(nb_out_filters, 1, 1, bool_apply_act = False)\n",
        "\n",
        "        self.conv_dxd_1 = DepthwiseConv2D(kernel_size = kernel_sz, strides = stride_sz, padding = \"same\", depth_multiplier = 1, \n",
        "                                          dilation_rate = dilation_rate, use_bias = False)\n",
        "        \n",
        "        self.batch_norm_layer = BatchNormalization(axis = -1, momentum = 0.99)\n",
        "        self.relu6_layer = ReLU(max_value = 6.0, threshold = 0.0)\n",
        "    \n",
        "    \n",
        "    def call(self, input_tensor, training = True):\n",
        "        \n",
        "        x = self.conv_pxp_1(input_tensor, training = training);\n",
        "        x = self.conv_dxd_1(x); x = self.batch_norm_layer(x, training = training); x = self.relu6_layer(x);\n",
        "        x = self.conv_pxp_2(x, training = training);\n",
        "        \n",
        "        if self.res_connect: x = x + input_tensor;\n",
        "\n",
        "        return x;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jnznh_d_8QH6",
        "colab": {}
      },
      "source": [
        "class InvertedResidual_Block(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, nb_repeats, nb_out_filters, nb_inp_filters, kernel_sz = 3, stride_sz = 1, dilation_rate = 1, exp_ratio = 6, \n",
        "                 width_multiplier = 1):\n",
        "        \n",
        "        super(InvertedResidual_Block, self).__init__(); self.nb_repeats = nb_repeats; self.model_layers = []; \n",
        "        \n",
        "        inv_res_layer = InvertedResidual(nb_out_filters, nb_inp_filters, kernel_sz, stride_sz, dilation_rate, exp_ratio, width_multiplier);\n",
        "        self.model_layers.append(inv_res_layer);\n",
        "        \n",
        "        for idx in range(1, self.nb_repeats):\n",
        "            inv_res_layer = InvertedResidual(nb_out_filters, nb_out_filters, kernel_sz, 1, dilation_rate, exp_ratio, width_multiplier);\n",
        "            self.model_layers.append(inv_res_layer);\n",
        "        \n",
        "    \n",
        "    def call(self, x, training = True):\n",
        "        \n",
        "        for idx in range(self.nb_repeats): \n",
        "            layer = self.model_layers[idx]; x = layer(x, training = training)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmJX-nm-X7SJ",
        "colab_type": "text"
      },
      "source": [
        "### MobileNet_v2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P_qk0Dri8Osi",
        "colab": {}
      },
      "source": [
        "class Dilated_MobileNet(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, out_stride = 8):\n",
        "        \n",
        "        super(Dilated_MobileNet, self).__init__(); self.out_stride = out_stride;\n",
        "        \n",
        "        self.conv_layer = Conv2D_BN(nb_out_filters = 32, kernel_sz = 3, stride_sz = 2)\n",
        "        \n",
        "        self.blk_1 = InvertedResidual_Block(nb_repeats = 1, nb_out_filters = 16, nb_inp_filters = 32, kernel_sz = 3,\n",
        "                                            stride_sz = 1, dilation_rate = 1, exp_ratio = 1, width_multiplier = 1);\n",
        "        \n",
        "        self.blk_2 = InvertedResidual_Block(nb_repeats = 2, nb_out_filters = 24, nb_inp_filters = 16, kernel_sz = 3,\n",
        "                                            stride_sz = 2, dilation_rate = 1, exp_ratio = 6, width_multiplier = 1);\n",
        "        \n",
        "        self.blk_3 = InvertedResidual_Block(nb_repeats = 3, nb_out_filters = 32, nb_inp_filters = 24, kernel_sz = 3,\n",
        "                                            stride_sz = 2, dilation_rate = 1, exp_ratio = 6, width_multiplier = 1);\n",
        "        \n",
        "        self.blk_4 = InvertedResidual_Block(nb_repeats = 4, nb_out_filters = 64, nb_inp_filters = 32, kernel_sz = 3,\n",
        "                                            stride_sz = 1, dilation_rate = 2, exp_ratio = 6, width_multiplier = 1);\n",
        "        \n",
        "        self.blk_5 = InvertedResidual_Block(nb_repeats = 3, nb_out_filters = 96, nb_inp_filters = 64, kernel_sz = 3,\n",
        "                                            stride_sz = 1, dilation_rate = 2, exp_ratio = 6, width_multiplier = 1);\n",
        "        \n",
        "        self.blk_6 = InvertedResidual_Block(nb_repeats = 3, nb_out_filters = 160, nb_inp_filters = 96, kernel_sz = 3,\n",
        "                                            stride_sz = 1, dilation_rate = 2, exp_ratio = 6, width_multiplier = 1);\n",
        "        \n",
        "        self.blk_7 = InvertedResidual_Block(nb_repeats = 1, nb_out_filters = 320, nb_inp_filters = 160, kernel_sz = 3,\n",
        "                                            stride_sz = 1, dilation_rate = 2, exp_ratio = 6, width_multiplier = 1);\n",
        "        \n",
        "    \n",
        "    def call(self, input_tensor, training = True):\n",
        "        \n",
        "        x = self.conv_layer(input_tensor, training = training);\n",
        "        x = self.blk_1(x, training = training); x = self.blk_2(x, training = training);\n",
        "        x = self.blk_3(x, training = training); x = self.blk_4(x, training = training);\n",
        "        x = self.blk_5(x, training = training); x = self.blk_6(x, training = training);\n",
        "        x = self.blk_7(x, training = training);\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ndI5m5XkrPCU"
      },
      "source": [
        "### DenseNet ASPP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WoLlDa6ifsSs",
        "colab": {}
      },
      "source": [
        "class DenseASPP_block(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, nb_inp_filters, nb_bottleneck_fiters, nb_out_filters, kernel_sz = 3, dilation_rate = 1, drop_rate = 0.1, \n",
        "                 momentum = 0.9997, start_bn = True):\n",
        "        \n",
        "        super(DenseASPP_block, self).__init__(); self.start_bn = start_bn; self.drop_rate = drop_rate;\n",
        "        if self.start_bn: self.bn_layer = BatchNormalization(axis = -1, momentum = momentum);\n",
        "        \n",
        "        self.relu6_layer = ReLU(max_value = 6.0, threshold = 0.0);\n",
        "        self.conv_layer_1 = Conv2D_BN(nb_out_filters = nb_bottleneck_fiters, kernel_sz = 1, stride_sz = 1, dilation_rate = 1, \n",
        "                                      momentum = momentum);\n",
        "        \n",
        "        self.conv_layer_2 = Conv2D_BN(nb_out_filters = nb_out_filters, kernel_sz = kernel_sz, stride_sz = 1, dilation_rate = dilation_rate, \n",
        "                                      bool_apply_act = False, bool_apply_norm = False);\n",
        "        if drop_rate: self.dropout_layer = Dropout(rate = drop_rate)\n",
        "        \n",
        "    \n",
        "    def call(self, x, training = True):\n",
        "        \n",
        "        if self.start_bn: x = self.bn_layer(x, training = training); \n",
        "        \n",
        "        x = self.relu6_layer(x); x = self.conv_layer_1(x, training = training); \n",
        "        x = self.conv_layer_2(x, training = training);\n",
        "        \n",
        "        if self.drop_rate: x = self.dropout_layer(x, training = training);\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GTtSDOYFfsSz",
        "colab": {}
      },
      "source": [
        "class DenseASPP(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, nb_inp_filters = 16, nb_bottleneck_filters = 128, nb_growth_rate = 64, drop_rate_conv = 0.1, \n",
        "                 drop_rate_fc = 0.1):\n",
        "        \n",
        "        super(DenseASPP, self).__init__(); self.drop_rate_fc = drop_rate_fc; self.base_model = Dilated_MobileNet();\n",
        "        \n",
        "        if drop_rate_fc: self.dropout_layer = Dropout(rate = drop_rate_fc);\n",
        "        self.upsample_layer = UpSampling2D(size = (8, 8), interpolation = \"bilinear\");\n",
        "        \n",
        "        self.ASPP_blk_1 = DenseASPP_block(nb_inp_filters + 0 * nb_growth_rate, nb_bottleneck_filters, nb_growth_rate, \n",
        "                                          dilation_rate = 3,  drop_rate = drop_rate_conv, start_bn = False)\n",
        "        self.ASPP_blk_2 = DenseASPP_block(nb_inp_filters + 1 * nb_growth_rate, nb_bottleneck_filters, nb_growth_rate, \n",
        "                                          dilation_rate = 6,  drop_rate = drop_rate_conv, start_bn = True)\n",
        "        self.ASPP_blk_3 = DenseASPP_block(nb_inp_filters + 2 * nb_growth_rate, nb_bottleneck_filters, nb_growth_rate,\n",
        "                                          dilation_rate = 12, drop_rate = drop_rate_conv, start_bn = True)\n",
        "        self.ASPP_blk_4 = DenseASPP_block(nb_inp_filters + 3 * nb_growth_rate, nb_bottleneck_filters, nb_growth_rate,\n",
        "                                          dilation_rate = 18, drop_rate = drop_rate_conv, start_bn = True)\n",
        "        self.ASPP_blk_5 = DenseASPP_block(nb_inp_filters + 4 * nb_growth_rate, nb_bottleneck_filters, nb_growth_rate,\n",
        "                                          dilation_rate = 24, drop_rate = drop_rate_conv, start_bn = True)\n",
        "        \n",
        "        self.conv_layer = Conv2D_BN(nb_out_filters = 1, kernel_sz = 1, stride_sz = 1, bool_apply_act = False, \n",
        "                                    bool_apply_norm = False)\n",
        "        \n",
        "        \n",
        "    def call(self, input_tensor, training = True):\n",
        "        \n",
        "        features = self.base_model(input_tensor, training = training);\n",
        "        \n",
        "        x = self.ASPP_blk_1(features, training = training)\n",
        "        features = tf.keras.layers.concatenate([x, features], axis = -1);\n",
        "        \n",
        "        x = self.ASPP_blk_2(features, training = training)\n",
        "        features = tf.keras.layers.concatenate([x, features], axis = -1);\n",
        "        \n",
        "        x = self.ASPP_blk_3(features, training = training)\n",
        "        features = tf.keras.layers.concatenate([x, features], axis = -1);\n",
        "        \n",
        "        x = self.ASPP_blk_4(features, training = training)\n",
        "        features = tf.keras.layers.concatenate([x, features], axis = -1);\n",
        "        \n",
        "        x = self.ASPP_blk_5(features, training = training)\n",
        "        features = tf.keras.layers.concatenate([x, features], axis = -1);\n",
        "        \n",
        "        if self.drop_rate_fc: features = self.dropout_layer(features, training = training);\n",
        "        x = self.conv_layer(features, training = training); \n",
        "        \n",
        "        x = self.upsample_layer(x);\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2nw5VInWrJK0"
      },
      "source": [
        "### Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKb069Gsy0Ka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root_dir = \"/content/drive/My Drive/Samsung_Ishaara/\";  BATCH_SIZE = 128;  IMG_SIZE = 128;\n",
        "training_data_dir = root_dir + \"Training_Data/\"; test_data_dir  = root_dir + \"Test_Data/\";\n",
        "ckpt_dir = root_dir + \"Cache/tf_ckpts/\"; output_dir = root_dir + \"Cache/Outputs/\";\n",
        "\n",
        "def extract(src_file_path = None, des_file_path = None):\n",
        "\n",
        "    with ZipFile(src_file_path, 'r') as zip:\n",
        "        print(f'Extracting the files located at: {src_file_path}');\n",
        "        zip.extractall(path = des_file_path); print('Done!');\n",
        "\n",
        "# Extracting Labels\n",
        "# extract(training_data_dir + \"Labels.zip\", training_data_dir);\n",
        "\n",
        "# # Extracting Images\n",
        "# extract(training_data_dir + \"Images/Set_1.zip\", training_data_dir + \"Images/\");\n",
        "# extract(training_data_dir + \"Images/Set_2.zip\", training_data_dir + \"Images/\");\n",
        "# extract(training_data_dir + \"Images/Set_3.zip\", training_data_dir + \"Images/\");\n",
        "# extract(training_data_dir + \"Images/Set_4.zip\", training_data_dir + \"Images/\");"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MytcHP621c9",
        "colab_type": "code",
        "outputId": "69922413-2393-41a2-847e-3fb9c104e924",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "def key_to_sort(x): return int(x[-12:-4])\n",
        "\n",
        "def check(file_path = None):\n",
        "\n",
        "    filenames = glob.glob(file_path + '*/*.jpg', recursive = True)\n",
        "    print(f\"Total files at the {file_path}: {len(filenames)}!!\")\n",
        "\n",
        "    return sorted(filenames, key = key_to_sort);\n",
        "\n",
        "label_filenames = check(training_data_dir + \"Labels/\"); \n",
        "set1_image_filenames = check(training_data_dir + \"Images/Set_1/\"); set2_image_filenames = check(training_data_dir + \"Images/Set_2/\");\n",
        "set3_image_filenames = check(training_data_dir + \"Images/Set_3/\"); set4_image_filenames = check(training_data_dir + \"Images/Set_4/\");"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total files at the /content/drive/My Drive/Samsung_Ishaara/Training_Data/Labels/: 32560!!\n",
            "Total files at the /content/drive/My Drive/Samsung_Ishaara/Training_Data/Images/Set_1/: 32560!!\n",
            "Total files at the /content/drive/My Drive/Samsung_Ishaara/Training_Data/Images/Set_2/: 32560!!\n",
            "Total files at the /content/drive/My Drive/Samsung_Ishaara/Training_Data/Images/Set_3/: 32560!!\n",
            "Total files at the /content/drive/My Drive/Samsung_Ishaara/Training_Data/Images/Set_4/: 32560!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSErd0M3X7Sf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Helper:\n",
        "    \n",
        "    def __init__(self): pass\n",
        "    \n",
        "    @staticmethod\n",
        "    def display(display_list, itr, epoch, save_dir):\n",
        "    \n",
        "        plt.figure(figsize = (15, 15)); title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
        "        for i in range(len(display_list)):\n",
        "\n",
        "            plt.subplot(1, len(display_list), i + 1); plt.title(title[i])\n",
        "            plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
        "            plt.axis('off')\n",
        "            \n",
        "        plt.savefig(save_dir + str(itr) + \"_\" + str(epoch) + '_output.png', bbox_inches = 'tight');\n",
        "        \n",
        "    \n",
        "    @staticmethod\n",
        "    def create_mask(x):\n",
        "\n",
        "        mask = 1/(1 + np.exp(-x)); mask = (mask > 0.5).astype(np.uint8);\n",
        "        return mask;\n",
        "\n",
        "\n",
        "    def show_predictions(self, model, image, mask, itr, epoch, save_dir, num = 1):\n",
        "\n",
        "            rand_int = np.random.randint(low = 0, high = image.shape[-1], size = 1)[0];\n",
        "            sample_img  = image[rand_int]; sample_mask = mask[rand_int];\n",
        "            \n",
        "            predictions = model(sample_img[tf.newaxis, ...]);\n",
        "            predictions = self.create_mask(predictions);\n",
        "            self.display([sample_img, sample_mask, predictions[0]], itr, epoch, save_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PxIw_P5KrWb3",
        "colab": {}
      },
      "source": [
        "class DataLoader:\n",
        "\n",
        "    def __init__(self):\n",
        " \n",
        "        self.train_img_dir = training_data_dir + \"Images/\"; self.test_img_dir = test_data_dir + \"Images/\";\n",
        "        self.train_lab_dir = training_data_dir + \"Labels/\";\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def key_to_sort(x): return int(x[-12:-4]);\n",
        " \n",
        "    def get_filenames(self, file_path): return list(sorted(glob.glob(file_path + '*/*.jpg', recursive = True), key = self.key_to_sort))\n",
        "\n",
        "\n",
        "    def get_training_data_filenames(self):\n",
        "\n",
        "        set1_img_filenames = tf.data.Dataset.from_tensor_slices(tf.constant(self.get_filenames(self.train_img_dir + \"Set_1/\")));\n",
        "        set2_img_filenames = tf.data.Dataset.from_tensor_slices(tf.constant(self.get_filenames(self.train_img_dir + \"Set_2/\")));\n",
        "        set3_img_filenames = tf.data.Dataset.from_tensor_slices(tf.constant(self.get_filenames(self.train_img_dir + \"Set_3/\")));\n",
        "        set4_img_filenames = tf.data.Dataset.from_tensor_slices(tf.constant(self.get_filenames(self.train_img_dir + \"Set_4/\")));\n",
        "        lab_filenames = tf.data.Dataset.from_tensor_slices(tf.constant(self.get_filenames(self.train_lab_dir)));\n",
        "\n",
        "        return set1_img_filenames, set2_img_filenames, set3_img_filenames, set4_img_filenames, lab_filenames\n",
        "\n",
        "\n",
        "    def get_test_data_filenames(self): return tf.data.Dataset.from_tensor_slices(tf.constant(self.get_filenames(self.test_img_dir)))\n",
        "\n",
        "\n",
        "    @tf.function\n",
        "    def training_data_parser(self, image_path, label_path):\n",
        "        \n",
        "        image = tf.image.decode_jpeg(tf.io.read_file(filename = image_path), channels = 3)\n",
        "        image = tf.divide(tf.cast(image, dtype = tf.float32), 255.0) \n",
        "        image = tf.image.resize(image, size = (IMG_SIZE, IMG_SIZE));\n",
        "        \n",
        "        label = tf.image.decode_jpeg(tf.io.read_file(filename = label_path), channels = 1)\n",
        "        label = tf.divide(tf.cast(label, dtype = tf.float32), 255.0)\n",
        "        label = tf.image.resize(label, size = (IMG_SIZE, IMG_SIZE));\n",
        "        \n",
        "        if tf.random.uniform(()) > 0.5:\n",
        "            image = tf.image.flip_left_right(image); label = tf.image.flip_left_right(label);\n",
        "        \n",
        "        return image, label\n",
        "    \n",
        "\n",
        "    @tf.function\n",
        "    def test_data_parser(self, image_path):\n",
        "        \n",
        "        image = tf.image.decode_jpeg(tf.io.read_file(filename = image_path), channels = 3)\n",
        "        image = tf.divide(tf.cast(image, dtype = tf.float32), 255.0)\n",
        "        image = tf.image.resize(image, size = (IMG_SIZE, IMG_SIZE))\n",
        "        \n",
        "        return image\n",
        "\n",
        "\n",
        "    def create_training_dataset(self):\n",
        "        \n",
        "        set1_img_files, set2_img_files, set3_img_files, set4_img_files, lab_files = self.get_training_data_filenames();\n",
        "        \n",
        "        set1_train_dataset = tf.data.Dataset.zip((set1_img_files, lab_files))\n",
        "        set2_train_dataset = tf.data.Dataset.zip((set2_img_files, lab_files))\n",
        "        set3_train_dataset = tf.data.Dataset.zip((set3_img_files, lab_files))\n",
        "        set4_train_dataset = tf.data.Dataset.zip((set4_img_files, lab_files))\n",
        "\n",
        "        train_dataset = set1_train_dataset; train_dataset = train_dataset.concatenate(set2_train_dataset); \n",
        "        train_dataset = train_dataset.concatenate(set3_train_dataset); train_dataset = train_dataset.concatenate(set4_train_dataset);\n",
        "        \n",
        "        train_dataset = train_dataset.shuffle(buffer_size = 50000).repeat(1);\n",
        "        train_dataset = train_dataset.map(self.training_data_parser, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
        "        train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE);\n",
        "        \n",
        "        return train_dataset\n",
        "\n",
        "\n",
        "    def create_test_dataset(self):\n",
        "        \n",
        "        test_dataset = self.get_test_data_filenames();\n",
        "        test_dataset = test_dataset.map(self.test_data_parser, num_parallel_calls = tf.data.experimental.AUTOTUNE);\n",
        "        test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE);\n",
        "        \n",
        "        return test_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0VmMZzyXrU7W",
        "colab": {}
      },
      "source": [
        "class Train:\n",
        "    \n",
        "    def __init__(self, nb_epochs, learning_rate = 1e-3, epsilon = 1e-7):\n",
        "        \n",
        "        self.nb_epochs = nb_epochs; self.epsilon = epsilon; self.learning_rate = learning_rate; self.model = DenseASPP();\n",
        "        self.train_opt = self.get_optimizer(); dataloader = DataLoader(); self.train_dataset = dataloader.create_training_dataset();\n",
        "        \n",
        "        if not os.path.exists(output_dir): os.makedirs(output_dir);\n",
        "        if not os.path.exists(ckpt_dir): os.makedirs(ckpt_dir);\n",
        "        \n",
        "        self.ckpt = tf.train.Checkpoint(step = tf.Variable(1), optimizer = self.train_opt, net = self.model)\n",
        "        self.manager = tf.train.CheckpointManager(self.ckpt, ckpt_dir, max_to_keep = 10);\n",
        "        self.ckpt.restore(self.manager.latest_checkpoint);\n",
        "        \n",
        "        if self.manager.latest_checkpoint: print(\"Restored from {}\".format(self.manager.latest_checkpoint))\n",
        "        else: print(\"Initializing from scratch.\")\n",
        "    \n",
        "    \n",
        "    @staticmethod\n",
        "    def get_loss(y_true, y_pred, pos_weight = 1.0):\n",
        "        return tf.nn.weighted_cross_entropy_with_logits(labels = y_true, logits = y_pred, pos_weight = pos_weight)\n",
        "    \n",
        "    \n",
        "    def get_optimizer(self):\n",
        "        return tf.keras.optimizers.Adam(learning_rate = self.learning_rate, epsilon = self.epsilon)\n",
        "    \n",
        "    \n",
        "    @tf.function\n",
        "    def train_one_step(self, x, y):\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            y_hat = self.model(x); train_loss = self.get_loss(y, y_hat, 3.0);\n",
        "            grads = tape.gradient(train_loss, self.model.trainable_variables);\n",
        "        \n",
        "        self.train_opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "        self.ckpt.step.assign_add(1);\n",
        "        \n",
        "        return train_loss\n",
        "        \n",
        "    \n",
        "    def __call__(self):\n",
        "        \n",
        "        train_loss_results = []; train_acc_results = []; helper = Helper(); glob_itr = 0;\n",
        "        for epoch in range(18, self.nb_epochs + 1):\n",
        "            \n",
        "            # if epoch % 2 == 0: self.learning_rate /= 2;\n",
        "            epoch_loss_avg = tf.keras.metrics.Mean(); epoch_accuracy = tf.keras.metrics.BinaryAccuracy();\n",
        "            \n",
        "            for x, y in self.train_dataset:\n",
        "                \n",
        "                train_loss = self.train_one_step(x, y); glob_itr += 1;\n",
        "                epoch_loss_avg(train_loss); epoch_accuracy(y, self.model(x));\n",
        "                \n",
        "                if glob_itr % 100 == 0: \n",
        "                    self.learning_rate /= 2;\n",
        "                    print(\"Iteration {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(glob_itr, epoch_loss_avg.result(), epoch_accuracy.result()))\n",
        "                    helper.show_predictions(self.model, x, y, glob_itr, epoch, output_dir);\n",
        "            \n",
        "            train_loss_results.append(epoch_loss_avg.result()); train_acc_results.append(epoch_accuracy.result());\n",
        "\n",
        "            print(\"Saved checkpoint for epoch {}: {}\".format(epoch, self.manager.save()))\n",
        "            print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch, epoch_loss_avg.result(), epoch_accuracy.result()))\n",
        "            \n",
        "        return train_loss_results, train_acc_results, self.model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uLtnA7S9DT4r",
        "colab": {}
      },
      "source": [
        "train = Train(nb_epochs = 25, learning_rate = 1e-3); train_loss, train_acc, model = train();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_77ewc8Cw1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vabh23MTX7Sr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# weighted binary cross entropy loss\n",
        "# tf.nn.weighted_cross_entropy_with_logits(labels, logits, pos_weight, name = None)\n",
        "# generate mask instead of heat maps\n",
        "# change in learning rate (callback)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}